{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "from byte_tokenizer import ByteTokenizer\n",
    "from class_gpt import GPT\n",
    "from class_textdataset import TextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# parameters of the model\n",
    "context_length = 16\n",
    "model_dim = 12  # dimensionality for embedding and attention\n",
    "num_blocks = 4  # number of repetitions of the transformer block\n",
    "num_heads = 4  # number of self attention instances, each with size model_dim // num_heads\n",
    "\n",
    "tokenizer = ByteTokenizer()\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "lr=3e-4  # learning rate for the gradient descent method\n",
    "checkpoints = sorted([f for f in os.listdir() if 'pth' in f])\n",
    "checkpoint_file = torch.load(checkpoints[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, it takes the txt file 'bon_jovi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39604\n"
     ]
    }
   ],
   "source": [
    "with open(\"bon_jovi.txt\", 'r', encoding='utf-8') as file:\n",
    "    test = file.read()\n",
    "print(len(test))  # 39604\n",
    "\n",
    "dataset = TextDataset(test, tokenizer, context_length)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embeddings): Embedding(258, 12)\n",
       "  (pos_embeddings): Embedding(16, 12)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (mhsa): MultiHeadedSelfAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHeadAttention(\n",
       "            (get_keys): Linear(in_features=12, out_features=3, bias=False)\n",
       "            (get_queries): Linear(in_features=12, out_features=3, bias=False)\n",
       "            (geet_values): Linear(in_features=12, out_features=3, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (first_ln): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      (second_ln): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): VanillaNeuralNetwork(\n",
       "        (fc1): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (fc2): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (mhsa): MultiHeadedSelfAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHeadAttention(\n",
       "            (get_keys): Linear(in_features=12, out_features=3, bias=False)\n",
       "            (get_queries): Linear(in_features=12, out_features=3, bias=False)\n",
       "            (geet_values): Linear(in_features=12, out_features=3, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (first_ln): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      (second_ln): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): VanillaNeuralNetwork(\n",
       "        (fc1): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (fc2): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (mhsa): MultiHeadedSelfAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHeadAttention(\n",
       "            (get_keys): Linear(in_features=12, out_features=3, bias=False)\n",
       "            (get_queries): Linear(in_features=12, out_features=3, bias=False)\n",
       "            (geet_values): Linear(in_features=12, out_features=3, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (first_ln): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      (second_ln): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): VanillaNeuralNetwork(\n",
       "        (fc1): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (fc2): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (mhsa): MultiHeadedSelfAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x SingleHeadAttention(\n",
       "            (get_keys): Linear(in_features=12, out_features=3, bias=False)\n",
       "            (get_queries): Linear(in_features=12, out_features=3, bias=False)\n",
       "            (geet_values): Linear(in_features=12, out_features=3, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (first_ln): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      (second_ln): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): VanillaNeuralNetwork(\n",
       "        (fc1): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (fc2): Linear(in_features=12, out_features=12, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "  (vocabulary_projection): Linear(in_features=12, out_features=258, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the checkpoint and continue training\n",
    "\n",
    "model = GPT(vocab_size, context_length, model_dim, num_blocks, num_heads)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = checkpoint_file \n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "\n",
    "new_lr = 0.001  # new learning rate\n",
    "\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = new_lr\n",
    "\n",
    "model.train()  # make sure to set back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 5.0596\n",
      "Epoch 1 | Loss 5.0596\n",
      "Epoch 2 | Loss 5.0596\n",
      "Epoch 3 | Loss 5.0596\n",
      "Epoch 4 | Loss 5.0596\n",
      "Epoch 5 | Loss 5.0596\n",
      "Epoch 6 | Loss 5.0596\n",
      "Epoch 7 | Loss 5.0596\n",
      "Epoch 8 | Loss 5.0596\n",
      "Epoch 9 | Loss 5.0596\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training loop\n",
    "for epoch in range(10):\n",
    "    for batch_x, batch_y in loader:\n",
    "        \n",
    "        # forward\n",
    "        logits = model(batch_x).squeeze()  # [batch, seq, vocab]\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), batch_y.view(-1))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} | Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis saves only the parameters (weights and biases).\\n\\nLightweight and flexible.\\n\\nTo load it, you need to recreate the model architecture first:\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model’s weights only\n",
    "gpt_decoder_weights_file = f'gpt_decoder_weights_{today}.pth'\n",
    "\n",
    "torch.save(model.state_dict(), gpt_decoder_weights_file )\n",
    "\n",
    "\"\"\"\n",
    "This saves only the parameters (weights and biases).\n",
    "\n",
    "Lightweight and flexible.\n",
    "\n",
    "To load it, you need to recreate the model architecture first:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the entire model (architecture + weights)\n",
    "gpt_decoder_full_file = f'decoder_full_{today}.pth'\n",
    "\n",
    "torch.save(model, gpt_decoder_full_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimizer state too. If you want to resume training:\n",
    "checkpoint_file = f'checkpoint_{today}.pth'\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,                    # last completed epoch\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss                       # optional, for logging\n",
    "}, checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Important Tips\n",
    "\n",
    "Always call model.train() after loading if you want to train.\n",
    "\n",
    "If you only want to evaluate, use model.eval().\n",
    "\n",
    "Make sure the model architecture and optimizer are created exactly as before.\n",
    "\n",
    "You don’t need pickle — torch.save / torch.load handles everything safely.\n",
    "\n",
    "You can change the learning rate (or other hyperparameters) after loading a checkpoint. The key is that the optimizer state (like momentum in Adam/SGD) will still be there, but you can override the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
