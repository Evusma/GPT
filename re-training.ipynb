{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte tokenizer.\n",
    "\n",
    "Tokenizer → encodes text to IDs.\n",
    "\n",
    "For training the model, the pipeline must: encode -> batch -> feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteTokenizer:\n",
    "    \"\"\"\n",
    "    UTF-8 byte tokenizer: every byte (0–255) is a token.\n",
    "    Reserve extra IDs for special tokens (eos, pad).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 258\n",
    "        self.eos_token_id = 256\n",
    "        self.pad_token_id = 257\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        b = text.encode(\"utf-8\", errors=\"ignore\")\n",
    "        return list(b) + [self.eos_token_id]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        b = bytes([i for i in ids if i < 256])\n",
    "        return b.decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33, 33, 33, 256]\n"
     ]
    }
   ],
   "source": [
    "# text to see how the ByteTokenizer works - encoding\n",
    "\n",
    "tokenizer = ByteTokenizer()\n",
    "\n",
    "tokens = tokenizer.encode(\"hello world!!!\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!!!\n"
     ]
    }
   ],
   "source": [
    "# text to see how the ByteTokenizer works - decoding\n",
    "\n",
    "text = tokenizer.decode(tokens)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset for GPT\n",
    "\n",
    "Dataset → creates input-target pairs for LM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, block_size=16):  #  here, block_size = context_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "        # Flatten all tokens into one big sequence\n",
    "        all_tokens = []\n",
    "        for t in texts:\n",
    "            all_tokens.extend(tokenizer.encode(t))\n",
    "\n",
    "        # chop the tokenized sequence into chunks of block_size\n",
    "        self.data = []\n",
    "        for i in range(0, max(1, len(all_tokens) - block_size)):\n",
    "            x = all_tokens[i : i + block_size]\n",
    "            y = all_tokens[i+1 : i + block_size+1]\n",
    "            \n",
    "            # pad if too short\n",
    "            if len(x) < block_size:\n",
    "                pad_len = block_size - len(x)\n",
    "                x = x + [tokenizer.pad_token_id] * pad_len\n",
    "                y = y + [tokenizer.pad_token_id] * pad_len\n",
    "            \n",
    "            self.data.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the dataset\n",
    "\n",
    "dataset = TextDataset('hello world', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))     # 1\n",
    "# print(dataset[0])       # (x,y) tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, context_length: int, model_dim: int, num_blocks: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embeddings = nn.Embedding(context_length, model_dim)\n",
    "        # number of iterations trough the transformer block\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(self.TransformerBlock(model_dim, num_heads))\n",
    "        self.final_ln = nn.LayerNorm(model_dim)\n",
    "        self.vocabulary_projection = nn.Linear(model_dim, vocab_size)\n",
    "                   \n",
    "    def forward(self, context):\n",
    "        # context: TensorType[int] -> TensorType[float]\n",
    "        torch.manual_seed(0)\n",
    "        # 🔍 Debug check\n",
    "        # print(\"min:\", context.min().item(), \"max:\", context.max().item(), \"vocab_size:\", self.token_embeddings.num_embeddings)\n",
    "        \n",
    "        token_embeds = self.token_embeddings(context)  # B, T, D\n",
    "        B, T, D = token_embeds.shape\n",
    "        pos_embeds = self.pos_embeddings(torch.arange(T))\n",
    "        total_embeddings = token_embeds + pos_embeds\n",
    "        \n",
    "        un_normalized = self.vocabulary_projection(self.final_ln(self.blocks(total_embeddings)))\n",
    "        probs = nn.functional.softmax(un_normalized, dim = -1)\n",
    "        return probs\n",
    "        \n",
    "    class TransformerBlock(nn.Module):\n",
    "        \n",
    "        class MultiHeadedSelfAttention(nn.Module):\n",
    "        \n",
    "            class SingleHeadAttention(nn.Module):\n",
    "                \n",
    "                def __init__(self, model_dim, head_size):\n",
    "                    super().__init__()\n",
    "                    torch.manual_seed(0)\n",
    "                    # not biases in the linear layers for getting the keys, queries and values of the tokens (for attention, better results)\n",
    "                    self.get_keys = nn.Linear(model_dim, head_size, bias=False)\n",
    "                    self.get_queries = nn.Linear(model_dim, head_size, bias=False)\n",
    "                    self.get_values = nn.Linear(model_dim, head_size, bias=False)\n",
    "\n",
    "                def forward(self, embedded):\n",
    "                    k = self.get_keys(embedded)  # BxTxA\n",
    "                    q = self.get_queries(embedded)\n",
    "                    v = self.get_values(embedded)\n",
    "\n",
    "                    scores = q @ torch.transpose(k, 1, 2)\n",
    "                    b, t, a = k.shape  # batch dim, context dim, attention dim\n",
    "                    scores = scores/ (a ** 0.5)\n",
    "\n",
    "                    # lower triangular tensor\n",
    "                    pre_mask = torch.tril(torch.ones(t, t))\n",
    "                    mask = pre_mask == 0\n",
    "\n",
    "                    scores = scores.masked_fill(mask, float('-inf'))  # b, t, t\n",
    "                    scores = nn.functional.softmax(scores, dim=2)  # dim=2 is the columns\n",
    "                    transformed = scores @ v\n",
    "\n",
    "                    return transformed\n",
    "        \n",
    "            def __init__(self, model_dim, num_heads):\n",
    "                super().__init__()\n",
    "                torch.manual_seed(0)\n",
    "                self.heads = nn.ModuleList()  # list to store neural network layers\n",
    "                for i in range(num_heads):\n",
    "                    # list of single attention layers\n",
    "                    self.heads.append(self.SingleHeadAttention(model_dim, model_dim // num_heads))\n",
    "\n",
    "            def forward(self, embedded):\n",
    "                outputs = []  # each element is B, T, Head_size --> B, T, Attention_sim (after concatenation)\n",
    "                for head in self.heads:\n",
    "                    outputs.append(head(embedded))\n",
    "                cated = torch.cat(outputs, dim = 2)  # dim = 2 the last dimension (attention)\n",
    "                return cated\n",
    "    \n",
    "        def __init__(self, model_dim, num_heads):\n",
    "            super().__init__()\n",
    "            torch.manual_seed(0)\n",
    "            # multi head self attention layer\n",
    "            self.mhsa = self.MultiHeadedSelfAttention(model_dim, num_heads)\n",
    "            # layers norm\n",
    "            self.first_ln = nn.LayerNorm(model_dim)\n",
    "            self.second_ln = nn.LayerNorm(model_dim)\n",
    "            # fee forward\n",
    "            self.ff = self.VanillaNeuralNetwork(model_dim)\n",
    "        \n",
    "\n",
    "        def forward(self, embedded):\n",
    "            # def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
    "            torch.manual_seed(0)\n",
    "            # add layer after the multi head self attention\n",
    "            first_part = embedded + self.mhsa(self.first_ln(embedded))\n",
    "            # add layer after feed forward\n",
    "            result = first_part + self.ff(self.second_ln(first_part))\n",
    "            return result\n",
    "    \n",
    "        class VanillaNeuralNetwork(nn.Module):\n",
    "        \n",
    "            def __init__(self, model_dim, droput=0.1):\n",
    "                super().__init__()\n",
    "                self.fc1 = nn.Linear(model_dim, model_dim)\n",
    "                self.fc2 = nn.Linear(model_dim, model_dim)\n",
    "                self.dropout = nn.Dropout()\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.fc1(x)\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc2(x)\n",
    "                x = self.dropout(x)\n",
    "                return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, it takes the txt file 'bon_jovi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39604\n"
     ]
    }
   ],
   "source": [
    "with open(\"bon_jovi.txt\", 'r', encoding='utf-8') as file:\n",
    "    test = file.read()\n",
    "print(len(test))  # 39604\n",
    "dataset = TextDataset(test, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(vocab_size=tokenizer.vocab_size, context_length=16, model_dim=12, num_blocks= 4, num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 5.0596\n",
      "Epoch 1 | Loss 5.0596\n",
      "Epoch 2 | Loss 5.0596\n",
      "Epoch 3 | Loss 5.0596\n",
      "Epoch 4 | Loss 5.0596\n",
      "Epoch 5 | Loss 5.0596\n",
      "Epoch 6 | Loss 5.0596\n",
      "Epoch 7 | Loss 5.0596\n",
      "Epoch 8 | Loss 5.0596\n",
      "Epoch 9 | Loss 5.0596\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(10):\n",
    "    for batch_x, batch_y in loader:\n",
    "        \n",
    "        # forward\n",
    "        logits = model(batch_x).squeeze()  # [batch, seq, vocab]\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), batch_y.view(-1))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} | Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis saves only the parameters (weights and biases).\\n\\nLightweight and flexible.\\n\\nTo load it, you need to recreate the model architecture first:\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model’s weights only\n",
    "\n",
    "# After training\n",
    "torch.save(model.state_dict(), \"gpt_decoder_weights.pth\")\n",
    "\n",
    "\"\"\"\n",
    "This saves only the parameters (weights and biases).\n",
    "\n",
    "Lightweight and flexible.\n",
    "\n",
    "To load it, you need to recreate the model architecture first:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model instance and load it\n",
    "model = GPT(vocab_size=tokenizer.vocab_size, context_length=16, model_dim=12, num_blocks= 4, num_heads=4)  # same arguments as during training\n",
    "model.load_state_dict(torch.load(\"gpt_decoder_weights.pth\"))\n",
    "model.eval()  # set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2️⃣ Saving the entire model (architecture + weights)\n",
    "\n",
    "torch.save(model, \"gpt_decoder_full.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "model = torch.load(\"gpt_decoder_full.pth\")\n",
    "model.eval()\n",
    "\n",
    "# Warning: Less flexible; may break if you change your code or PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ (Optional) Save optimizer state too. If you want to resume training:\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,                    # last completed epoch\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss                       # optional, for logging\n",
    "}, \"checkpoint_2025_09_12.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint and continue training\n",
    "\n",
    "model = GPT(vocab_size=tokenizer.vocab_size, context_length=16, model_dim=12, num_blocks= 4, num_heads=4)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"checkpoint_2025_09_12.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "\n",
    "new_lr = 0.0001  # new learning rate\n",
    "\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = new_lr\n",
    "\n",
    "model.train()  # make sure to set back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Important Tips\n",
    "\n",
    "Always call model.train() after loading if you want to train.\n",
    "\n",
    "If you only want to evaluate, use model.eval().\n",
    "\n",
    "Make sure the model architecture and optimizer are created exactly as before.\n",
    "\n",
    "You don’t need pickle — torch.save / torch.load handles everything safely.\n",
    "\n",
    "You can change the learning rate (or other hyperparameters) after loading a checkpoint. The key is that the optimizer state (like momentum in Adam/SGD) will still be there, but you can override the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
