{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GPT](./image.png)\n",
    "\n",
    "The GPT has two embedding layers: \n",
    "- Token embedding layer (vocabulary size, embeding dim)\n",
    "- Positional embedding layer (context length, embeding dim), how many tokens back in the sequence can the model read\n",
    "\n",
    "They are there to learn or train feature vectors for every sinlge token in our vocabulary\n",
    "\n",
    "The result of both layers, token and positional embedding layers, is sent to the transformer.\n",
    "The output of the transformer can be sent to the transformer again n times to get a more complex model (using nn.sequential as a python list of NN layers)\n",
    "\n",
    "After the transformer block, before linear layer (vacabulary projection layer as the input is the vocabulary size) and softmax layer (probabilities), it can be used an additional norm layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT\n",
    "\n",
    "Ready to code the GPT class. This follows the architecture that almost all large language models use.\n",
    "\n",
    "The output return a tensor (batch size, context_length, vocabulary size)\n",
    "The output layer has vocabulary size neurons, each representing the likelihhod of a particular toking coming next\n",
    "\n",
    "inputs:\n",
    "- vocabulary size: number of different tokens the model recognizes\n",
    "- context_length: how many tokens back the model can read\n",
    "- model_dim: feature dimensionality for embedding and attention\n",
    "- num_blocks: number of repetitions of the transformer block\n",
    "- num_heads: number of self attention instances\n",
    "- context: previous tokens used to make the prediction\n",
    "\n",
    "input to the forward method is size B (batch) T (context length) \n",
    "we train these neural network in batches (B batch: how many sequences we are passing in parallel)\n",
    "\n",
    "in the last layer of the neural network we have a size B (batch) T (context length) V (vocabulary size)\n",
    "for every context of the input, the model is generating a vector of size vocabulary -> list of probabilities, a probability for every possible token of what could come next\n",
    "\n",
    "from b t d to b t v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, context_length: int, model_dim: int, num_blocks: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(0)\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embeddings = nn.Embedding(context_length, model_dim)\n",
    "        # number of iterations trough the transformer block\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(num_blocks):\n",
    "            self.blocks.append(self.TransformerBlock(model_dim, num_heads))\n",
    "        self.final_ln = nn.LayerNorm(model_dim)\n",
    "        self.vocabulary_projection = nn.Linear(model_dim, vocab_size)\n",
    "                   \n",
    "    def forward(self, context):\n",
    "        # context: TensorType[int] -> TensorType[float]\n",
    "        torch.manual_seed(0)\n",
    "        token_embeds = self.token_embeddings(context)  # B, T, D\n",
    "        B, T, D = token_embeds.shape\n",
    "        pos_embeds = self.pos_embeddings(torch.arange(T))\n",
    "        total_embeddings = token_embeds + pos_embeds\n",
    "        \n",
    "        un_normalized = self.vocabulary_projection(self.final_ln(self.blocks(total_embeddings)))\n",
    "        probs = nn.functional.softmax(un_normalized, dim = -1)\n",
    "        return probs\n",
    "        \n",
    "    class TransformerBlock(nn.Module):\n",
    "        \n",
    "        class MultiHeadedSelfAttention(nn.Module):\n",
    "        \n",
    "            class SingleHeadAttention(nn.Module):\n",
    "                \n",
    "                def __init__(self, model_dim, head_size):\n",
    "                    super().__init__()\n",
    "                    torch.manual_seed(0)\n",
    "                    # not biases in the linear layers for getting the keys, queries and values of the tokens (for attention, better results)\n",
    "                    self.get_keys = nn.Linear(model_dim, head_size, bias=False)\n",
    "                    self.get_queries = nn.Linear(model_dim, head_size, bias=False)\n",
    "                    self.geet_values = nn.Linear(model_dim, head_size, bias=False)\n",
    "\n",
    "                def forward(self, embedded):\n",
    "                    k = self.get_keys(embedded)  # BxTxA\n",
    "                    q = self.get_queries(embedded)\n",
    "                    v = self.geet_values(embedded)\n",
    "\n",
    "                    scores = q @ torch.transpose(k, 1, 2)\n",
    "                    b, t, a = k.shape  # batch dim, context dim, attention dim\n",
    "                    scores = scores/ (a ** 0.5)\n",
    "\n",
    "                    # lower triangular tensor\n",
    "                    pre_mask = torch.tril(torch.ones(t, t))\n",
    "                    mask = pre_mask == 0\n",
    "\n",
    "                    scores = scores.masked_fill(mask, float('-inf'))  # b, t, t\n",
    "                    scores = nn.functional.softmax(scores, dim=2)  # dim=2 is the columns\n",
    "                    transformed = scores @ v\n",
    "\n",
    "                    return transformed\n",
    "        \n",
    "            def __init__(self, model_dim, num_heads):\n",
    "                super().__init__()\n",
    "                torch.manual_seed(0)\n",
    "                self.heads = nn.ModuleList()  # list to store neural network layers\n",
    "                for i in range(num_heads):\n",
    "                    # list of single attention layers\n",
    "                    self.heads.append(self.SingleHeadAttention(model_dim, model_dim // num_heads))\n",
    "\n",
    "            def forward(self, embedded):\n",
    "                outputs = []  # each element is B, T, Head_size --> B, T, Attention_sim (after concatenation)\n",
    "                for head in self.heads:\n",
    "                    outputs.append(head(embedded))\n",
    "                cated = torch.cat(outputs, dim = 2)  # dim = 2 the last dimension (attention)\n",
    "                return cated\n",
    "    \n",
    "        def __init__(self, model_dim, num_heads):\n",
    "            super().__init__()\n",
    "            torch.manual_seed(0)\n",
    "            # multi head self attention layer\n",
    "            self.mhsa = self.MultiHeadedSelfAttention(model_dim, num_heads)\n",
    "            # layers norm\n",
    "            self.first_ln = nn.LayerNorm(model_dim)\n",
    "            self.second_ln = nn.LayerNorm(model_dim)\n",
    "            # fee forward\n",
    "            self.ff = self.VanillaNeuralNetwork(model_dim)\n",
    "        \n",
    "\n",
    "        def forward(self, embedded):\n",
    "            # def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
    "            torch.manual_seed(0)\n",
    "            # add layer after the multi head self attention\n",
    "            first_part = embedded + self.mhsa(self.first_ln(embedded))\n",
    "            # add layer after feed forward\n",
    "            result = first_part + self.ff(self.second_ln(first_part))\n",
    "            return result\n",
    "    \n",
    "        class VanillaNeuralNetwork(nn.Module):\n",
    "        \n",
    "            def __init__(self, model_dim, droput=0.1):\n",
    "                super().__init__\n",
    "                self.fc1 = nn.Linear(model_dim, model_dim)\n",
    "                self.fc2 = nn.Linear(model_dim, model_dim)\n",
    "                self.dropout = nn.Dropout()\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.fc1(x)\n",
    "                x = F.relu(x)\n",
    "                x = self.dropout(x)\n",
    "                x = self.fc2(x)\n",
    "                x = self.dropout(x)\n",
    "                return x\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
